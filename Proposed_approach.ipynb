{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proposed approach.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "RyMqv-4p87Mj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWpMmb5LBQ2v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dch_23XG9Guv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clone repository required for this coursework\n",
        "!git clone https://github.com/MatchLab-Imperial/keras_triplet_descriptor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5wPCypm9TDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Change directory\n",
        "%cd /content/keras_triplet_descriptor "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dultp26r9W9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download data\n",
        "!wget -O hpatches_data.zip https://imperialcollegelondon.box.com/shared/static/ah40eq7cxpwq4a6l4f62efzdyt8rm3ha.zip\n",
        "# Extract data\n",
        "!unzip -q ./hpatches_data.zip\n",
        "!rm ./hpatches_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRQTupLX9XFU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import necessary modules\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization \n",
        "from keras.layers import Input, UpSampling2D, concatenate  \n",
        "\n",
        "from read_data import HPatches, DataGeneratorDesc, hpatches_sequence_folder, DenoiseHPatches, tps\n",
        "from utils import generate_desc_csv, plot_denoise, plot_triplet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhGd_6Gt9XMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize random seed\n",
        "\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ih-5ej09XQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "\n",
        "hpatches_dir = './hpatches'\n",
        "splits_path = './splits.json'\n",
        "\n",
        "splits_json = json.load(open(splits_path, 'rb'))\n",
        "split = splits_json['a']\n",
        "\n",
        "train_fnames = split['train']\n",
        "test_fnames = split['test']\n",
        "\n",
        "seqs = glob.glob(hpatches_dir+'/*')\n",
        "seqs = [os.path.abspath(p) for p in seqs]   \n",
        "seqs_train = list(filter(lambda x: x.split('/')[-1] in train_fnames, seqs)) \n",
        "seqs_test = list(filter(lambda x: x.split('/')[-1] in split['test'], seqs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lKOB9-Iv9XT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model for proposed approach of denoising network\n",
        "\n",
        "def get_denoise_model(shape):\n",
        "    \n",
        "  inputs = Input(shape)\n",
        "  \n",
        "  ## Encoder starts\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  \n",
        "  conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  conv4 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "  \n",
        "  ## Bottleneck\n",
        "  conv5 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up1 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv5))\n",
        "  merge1 = concatenate([conv4,up1], axis = -1)\n",
        "  deconv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
        "  deconv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(deconv1)\n",
        "  \n",
        "  up2 = Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(deconv2))\n",
        "  merge2 = concatenate([conv2,up2], axis = -1)\n",
        "  deconv3 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
        "  deconv4 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(deconv3)\n",
        "  \n",
        "  deconv5 = Conv2D(1, 3,  padding = 'same')(deconv4)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = deconv5)\n",
        "  \n",
        "  return shallow_net\n",
        "\n",
        "#shallow UNet from the baseline approach\n",
        "def get_denoise_model_1(shape):\n",
        "    \n",
        "  inputs = Input(shape)\n",
        "  \n",
        "  ## Encoder starts\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  \n",
        "  ## Bottleneck\n",
        "  conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up3 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n",
        "  merge3 = concatenate([conv1,up3], axis = -1)\n",
        "  conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
        "    \n",
        "  conv4 = Conv2D(1, 3,  padding = 'same')(conv3)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = conv4)\n",
        "  \n",
        "  return shallow_net\n",
        "\n",
        "#3-layered UNet as comparison with proposed approach\n",
        "\n",
        "def get_denoise_model_3(shape):\n",
        "    \n",
        "  inputs = Input(shape)\n",
        "  \n",
        "  ## Encoder starts\n",
        "  conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  \n",
        "  conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  conv4 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "  \n",
        "  conv5 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "  conv6 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
        "  \n",
        "  \n",
        "  ## Bottleneck\n",
        "  conv7 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "\n",
        "  ## Now the decoder starts\n",
        "  up1 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "  merge1 = concatenate([conv6,up1], axis = -1)\n",
        "  deconv7 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
        "  deconv6 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(deconv7)\n",
        "  \n",
        "  up2 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(deconv6))\n",
        "  merge2 = concatenate([conv4,up2], axis = -1)\n",
        "  deconv5 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
        "  deconv4 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(deconv5)\n",
        "  \n",
        "  up3 = Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(deconv4))\n",
        "  merge3 = concatenate([conv2,up3], axis = -1)\n",
        "  deconv3 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
        "  deconv2 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(deconv3)\n",
        "    \n",
        "  deconv1 = Conv2D(1, 3,  padding = 'same')(deconv2)\n",
        "\n",
        "  shallow_net = Model(inputs = inputs, outputs = deconv1)\n",
        "  \n",
        "  return shallow_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_yT3x_tj9XV9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Proposed approach does not modify the architecture but the alpha value in triples loss function\n",
        "\n",
        "def get_descriptor_model(shape):\n",
        "  \n",
        "  '''Architecture copies HardNet architecture'''\n",
        "  \n",
        "  init_weights = keras.initializers.he_normal()\n",
        "  \n",
        "  descriptor_model = Sequential()\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', input_shape=shape, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(32, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', strides=2, use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(64, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', strides=2,  use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 3, padding='same', use_bias = True, kernel_initializer=init_weights))\n",
        "  descriptor_model.add(BatchNormalization(axis = -1))\n",
        "  descriptor_model.add(Activation('relu'))\n",
        "  descriptor_model.add(Dropout(0.3))\n",
        "\n",
        "  descriptor_model.add(Conv2D(128, 8, padding='valid', use_bias = True, kernel_initializer=init_weights))\n",
        "  \n",
        "  # Final descriptor reshape\n",
        "  descriptor_model.add(Reshape((128,)))\n",
        "  \n",
        "  return descriptor_model\n",
        "  \n",
        "  \n",
        "def triplet_loss(x):\n",
        "  \n",
        "  output_dim = 128\n",
        "  a, p, n = x\n",
        "  _alpha = 5.0\n",
        "  positive_distance = K.mean(K.square(a - p), axis=-1)\n",
        "  negative_distance = K.mean(K.square(a - n), axis=-1)\n",
        "  \n",
        "  return K.expand_dims(K.maximum(0.0, positive_distance - negative_distance + _alpha), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Uk-Pqxh-mfw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function that plots the learning curve\n",
        "\n",
        "def plot_history(history, metric = None):\n",
        "  # Plots the loss history of training and validation (if existing)\n",
        "  # and a given metric\n",
        "  \n",
        "  if metric != None:\n",
        "    fig, axes = plt.subplots(2,1)\n",
        "    axes[0].plot(history.history[metric])\n",
        "    try:\n",
        "      axes[0].plot(history.history['val_'+metric])\n",
        "      axes[0].legend(['Train', 'Val'])\n",
        "    except:\n",
        "      pass\n",
        "    axes[0].set_title('{:s}'.format(metric))\n",
        "    axes[0].set_ylabel('{:s}'.format(metric))\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    axes[1].plot(history.history['loss'])\n",
        "    try:\n",
        "      axes[1].plot(history.history['val_loss'])\n",
        "      axes[1].legend(['Train', 'Val'])\n",
        "    except:\n",
        "      pass\n",
        "    axes[1].set_title('Model Loss')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "  else:\n",
        "    plt.plot(history.history['loss'])\n",
        "    try:\n",
        "      plt.plot(history.history['val_loss'])\n",
        "      plt.legend(['Train', 'Val'])\n",
        "    except:\n",
        "      pass\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2BG0Pbo-miQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the input\n",
        "\n",
        "denoise_generator = DenoiseHPatches(seqs_train, batch_size=50)\n",
        "denoise_generator_val = DenoiseHPatches(seqs_test, batch_size=50)\n",
        "\n",
        "# Following code is used for testing differetn set-up\n",
        "# denoise_generator = DenoiseHPatches(random.sample(seqs_train, 3), batch_size=50)\n",
        "# denoise_generator_val = DenoiseHPatches(random.sample(seqs_test, 1), batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2BtJvKM-mkp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shape = (32, 32, 1)\n",
        "denoise_model = get_denoise_model(shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b5PuhZ5s-mm6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# optimizers tested are listed below\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.00001, momentum=0.9, nesterov=True)\n",
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "RMS = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "\n",
        "denoise_model.compile(loss='mean_absolute_error', optimizer=adam, metrics=['mae'])\n",
        "\n",
        "denoise_history = denoise_model.fit_generator(generator=denoise_generator, \n",
        "                                                epochs=50, verbose=1, \n",
        "                                                validation_data=denoise_generator_val)\n",
        "# Saves optimizer and weights\n",
        "denoise_model.save('denoise.h5') \n",
        "# Uploads files to external hosting\n",
        "!curl -F \"file=@denoise.h5\" https://file.io\n",
        "    \n",
        "plot_history(denoise_history,'mean_absolute_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ox2IoQVX-mpK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build descriptor network\n",
        "\n",
        "from keras.layers import Lambda\n",
        "shape = (32, 32, 1)\n",
        "xa = Input(shape=shape, name='a')\n",
        "xp = Input(shape=shape, name='p')\n",
        "xn = Input(shape=shape, name='n')\n",
        "descriptor_model = get_descriptor_model(shape)\n",
        "ea = descriptor_model(xa)\n",
        "ep = descriptor_model(xp)\n",
        "en = descriptor_model(xn)\n",
        "\n",
        "loss = Lambda(triplet_loss)([ea, ep, en])\n",
        "\n",
        "descriptor_model_trip = Model(inputs=[xa, xp, xn], outputs=loss)\n",
        "sgd = keras.optimizers.SGD(lr=0.1)\n",
        "\n",
        "adam = keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "descriptor_model_trip.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "axdq_Xxy-mrQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Descriptor loading and training\n",
        "# Loading images\n",
        "hPatches = HPatches(train_fnames=train_fnames, test_fnames=test_fnames,\n",
        "                    denoise_model=denoise_model, use_clean=False)\n",
        "# Creating training generator\n",
        "training_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=1), num_triplets=100000)\n",
        "# Creating validation generator\n",
        "val_generator = DataGeneratorDesc(*hPatches.read_image_file(hpatches_dir, train=0), num_triplets=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6FqyvnQ2-mts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training of the descriptor network\n",
        "descriptor_history = descriptor_model_trip.fit_generator(generator=training_generator, epochs=10, verbose=1, validation_data=val_generator)\n",
        "  ### Saves optimizer and weights\n",
        "descriptor_model_trip.save('descriptor.h5') \n",
        "  ### Uploads files to external hosting\n",
        "!curl -F \"file=@descriptor.h5\" https://file.io\n",
        "descriptor_model_trip.save_weights('descriptor.h5') \n",
        "\n",
        "plot_history(descriptor_history,'mean_absolute_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7dfon62-mv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#generate csv file\n",
        "generate_desc_csv(descriptor_model, seqs_test, denoise_model=denoise_model, use_clean=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLRZmRao-myU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Patch verification\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=verification --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=verification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qyyjJmfHArqX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Image matching \n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=matching --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=matching"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XiY1LgpLArvm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Patch retrieval\n",
        "\n",
        "!python ./hpatches-benchmark/hpatches_eval.py --descr-name=custom --descr-dir=/content/keras_triplet_descriptor/out/ --task=retrieval --delimiter=\";\"\n",
        "!python ./hpatches-benchmark/hpatches_results.py --descr=custom --results-dir=./hpatches-benchmark/results/ --task=retrieval"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}